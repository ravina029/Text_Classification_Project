{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6d40805",
   "metadata": {},
   "source": [
    "# 03_Feature_Engineering_and_Modeling\n",
    "\n",
    "PhD-level notebook: TF-IDF baselines + DistilBERT fine-tuning for IMDB sentiment classification.\n",
    "\n",
    "Saves outputs (models, vectorizers, metrics) to the `results/` folder at project root.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce868d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.12.4 (main, Jun  6 2024, 18:26:44) [Clang 15.0.0 (clang-1500.3.9.4)]\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "set -e\n",
    "python -c \"import sys; print('Python', sys.version)\"\n",
    "# Install datasets if missing (transformers assumed installed)\n",
    "python -c \"import importlib; importlib.import_module('datasets')\" 2>/dev/null || pip install --quiet datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6da11d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed path: /Users/ravina/Desktop/Text_Classification_Project/data/processed/imdb_preprocessed.csv\n",
      "Results dir: /Users/ravina/Desktop/Text_Classification_Project/results\n",
      "Loaded rows: 50000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>split</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This movie is another Christian propaganda fil...</td>\n",
       "      <td>neg</td>\n",
       "      <td>train</td>\n",
       "      <td>this movie is another christian propaganda fil...</td>\n",
       "      <td>movie another christian propaganda film line o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This show has been performed live around the c...</td>\n",
       "      <td>neg</td>\n",
       "      <td>test</td>\n",
       "      <td>this show has been performed live around the c...</td>\n",
       "      <td>show perform live around country wide variety ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Beast Wars is a show that is over-hyped, overp...</td>\n",
       "      <td>neg</td>\n",
       "      <td>train</td>\n",
       "      <td>beast wars is a show that is over hyped overpr...</td>\n",
       "      <td>beast war show hype overpraise overrate let me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This is absolutely beyond question the worst m...</td>\n",
       "      <td>neg</td>\n",
       "      <td>test</td>\n",
       "      <td>this is absolutely beyond question the worst m...</td>\n",
       "      <td>absolutely beyond question bad movie ever see ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A box with a button provides a couple with the...</td>\n",
       "      <td>neg</td>\n",
       "      <td>test</td>\n",
       "      <td>a box with a button provides a couple with the...</td>\n",
       "      <td>box button provide couple opportunity financia...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment  split  \\\n",
       "0  This movie is another Christian propaganda fil...       neg  train   \n",
       "1  This show has been performed live around the c...       neg   test   \n",
       "2  Beast Wars is a show that is over-hyped, overp...       neg  train   \n",
       "3  This is absolutely beyond question the worst m...       neg   test   \n",
       "4  A box with a button provides a couple with the...       neg   test   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0  this movie is another christian propaganda fil...   \n",
       "1  this show has been performed live around the c...   \n",
       "2  beast wars is a show that is over hyped overpr...   \n",
       "3  this is absolutely beyond question the worst m...   \n",
       "4  a box with a button provides a couple with the...   \n",
       "\n",
       "                                      processed_text  \n",
       "0  movie another christian propaganda film line o...  \n",
       "1  show perform live around country wide variety ...  \n",
       "2  beast war show hype overpraise overrate let me...  \n",
       "3  absolutely beyond question bad movie ever see ...  \n",
       "4  box button provide couple opportunity financia...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Paths (adjust if your project root differs)\n",
    "PREPROCESSED_PATH = Path('/Users/ravina/Desktop/Text_Classification_Project/data/processed/imdb_preprocessed.csv')\n",
    "RESULTS_DIR = Path('/Users/ravina/Desktop/Text_Classification_Project/results')\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print('Preprocessed path:', PREPROCESSED_PATH)\n",
    "print('Results dir:', RESULTS_DIR)\n",
    "\n",
    "assert PREPROCESSED_PATH.exists(), f'Preprocessed file not found at {PREPROCESSED_PATH}'\n",
    "df = pd.read_csv(PREPROCESSED_PATH)\n",
    "print('Loaded rows:', len(df))\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce36f3a",
   "metadata": {},
   "source": [
    "## Quick checks and train/test split\n",
    "Ensure your dataframe contains a processed text column. The notebook will try `processed_text`, `final_review`, `clean_text`, then `review`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9422cb64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using text column: processed_text\n",
      "Train samples: 40000 Test samples: 10000\n"
     ]
    }
   ],
   "source": [
    "# Choose text column\n",
    "for col in ['processed_text','final_review','clean_text','review']:\n",
    "    if col in df.columns:\n",
    "        TEXT_COL = col\n",
    "        break\n",
    "print('Using text column:', TEXT_COL)\n",
    "\n",
    "if df['sentiment'].dtype == 'object':\n",
    "    df['sentiment'] = df['sentiment'].map({'neg':0,'pos':1})\n",
    "\n",
    "X = df[TEXT_COL].astype(str)\n",
    "y = df['sentiment'].astype(int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "print('Train samples:', len(X_train), 'Test samples:', len(X_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5c483e",
   "metadata": {},
   "source": [
    "## TF-IDF vectorization and Classical ML Baselines\n",
    "We vectorize with TF-IDF and train Logistic Regression, MultinomialNB, LinearSVC, RandomForest.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2662979c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, confusion_matrix\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afa4d276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF shape: (40000, 10000)\n"
     ]
    }
   ],
   "source": [
    "vec = TfidfVectorizer(max_features=10000, ngram_range=(1,2))\n",
    "X_train_vec = vec.fit_transform(X_train)\n",
    "X_test_vec = vec.transform(X_test)\n",
    "print('TF-IDF shape:', X_train_vec.shape)\n",
    "\n",
    "models = {\n",
    "    'LogisticRegression': LogisticRegression(max_iter=1000),\n",
    "    'MultinomialNB': MultinomialNB(),\n",
    "    'LinearSVC': LinearSVC(max_iter=10000),\n",
    "    'RandomForest': RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=42)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e23957b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training LogisticRegression\n",
      "LogisticRegression Accuracy: 0.8978 AUC: 0.96256068\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.89      0.90      5000\n",
      "           1       0.89      0.91      0.90      5000\n",
      "\n",
      "    accuracy                           0.90     10000\n",
      "   macro avg       0.90      0.90      0.90     10000\n",
      "weighted avg       0.90      0.90      0.90     10000\n",
      "\n",
      "\n",
      "Training MultinomialNB\n",
      "MultinomialNB Accuracy: 0.8714 AUC: 0.9411068000000001\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.86      0.87      5000\n",
      "           1       0.86      0.89      0.87      5000\n",
      "\n",
      "    accuracy                           0.87     10000\n",
      "   macro avg       0.87      0.87      0.87     10000\n",
      "weighted avg       0.87      0.87      0.87     10000\n",
      "\n",
      "\n",
      "Training LinearSVC\n",
      "LinearSVC Accuracy: 0.8913 AUC: None\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.89      0.89      5000\n",
      "           1       0.89      0.89      0.89      5000\n",
      "\n",
      "    accuracy                           0.89     10000\n",
      "   macro avg       0.89      0.89      0.89     10000\n",
      "weighted avg       0.89      0.89      0.89     10000\n",
      "\n",
      "\n",
      "Training RandomForest\n",
      "RandomForest Accuracy: 0.8563 AUC: 0.931542\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.87      0.86      5000\n",
      "           1       0.86      0.85      0.85      5000\n",
      "\n",
      "    accuracy                           0.86     10000\n",
      "   macro avg       0.86      0.86      0.86     10000\n",
      "weighted avg       0.86      0.86      0.86     10000\n",
      "\n",
      "\n",
      "Saved models and vectorizer to /Users/ravina/Desktop/Text_Classification_Project/results\n"
     ]
    }
   ],
   "source": [
    "\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    print('\\nTraining', name)\n",
    "    model.fit(X_train_vec, y_train)\n",
    "    preds = model.predict(X_test_vec)\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    try:\n",
    "        prob = model.predict_proba(X_test_vec)[:,1]\n",
    "        auc = roc_auc_score(y_test, prob)\n",
    "    except Exception:\n",
    "        auc = None\n",
    "    print(name, 'Accuracy:', acc, 'AUC:', auc)\n",
    "    print(classification_report(y_test, preds))\n",
    "    results[name] = {'model': model, 'accuracy': acc, 'auc': auc}\n",
    "    joblib.dump(model, RESULTS_DIR / f'{name}.joblib')\n",
    "\n",
    "# save vectorizer\n",
    "joblib.dump(vec, RESULTS_DIR / 'tfidf_vectorizer.joblib')\n",
    "print('\\nSaved models and vectorizer to', RESULTS_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a24851",
   "metadata": {},
   "source": [
    "### Feature importance from Logistic Regression (top features)\n",
    "We print top positive and negative coefficients from Logistic Regression for interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bc82483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top positive features:\n",
      "['great' 'excellent' 'perfect' 'amazing' 'wonderful' 'brilliant'\n",
      " 'favorite' 'today' 'love' 'good' 'fun' 'enjoy' 'superb' 'hilarious'\n",
      " 'enjoyable' 'must see' 'highly recommend' 'fantastic' 'one good'\n",
      " 'well worth' 'entertaining' 'highly' 'gem' 'perfectly' 'well do' 'solid'\n",
      " 'beautifully' 'definitely' 'incredible' 'wonderfully']\n",
      "\n",
      "Top negative features:\n",
      "['bad' 'waste' 'awful' 'poor' 'boring' 'terrible' 'nothing' 'dull' 'fail'\n",
      " 'horrible' 'poorly' 'unfortunately' 'disappointment' 'suppose' 'stupid'\n",
      " 'script' 'disappointed' 'disappointing' 'minute' 'lack' 'lame' 'instead'\n",
      " 'save' 'ridiculous' 'annoying' 'waste time' 'pointless' 'sorry' 'cheap'\n",
      " 'predictable']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "clf = results['LogisticRegression']['model']\n",
    "if hasattr(clf, 'coef_'):\n",
    "    coefs = clf.coef_[0]\n",
    "    top_pos_idx = np.argsort(coefs)[-30:][::-1]\n",
    "    top_neg_idx = np.argsort(coefs)[:30]\n",
    "    feature_names = np.array(vec.get_feature_names_out())\n",
    "    print('Top positive features:')\n",
    "    print(feature_names[top_pos_idx])\n",
    "    print('\\nTop negative features:')\n",
    "    print(feature_names[top_neg_idx])\n",
    "else:\n",
    "    print('Model has no coef_')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a9f53e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved classical results summary\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "summary = {k: {'accuracy': v['accuracy'], 'auc': v['auc']} for k,v in results.items()}\n",
    "with open(RESULTS_DIR / 'classical_results_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "print('Saved classical results summary')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ccceee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LogisticRegression': {'accuracy': 0.8978, 'auc': 0.96256068},\n",
       " 'MultinomialNB': {'accuracy': 0.8714, 'auc': 0.9411068000000001},\n",
       " 'LinearSVC': {'accuracy': 0.8913, 'auc': None},\n",
       " 'RandomForest': {'accuracy': 0.8563, 'auc': 0.931542}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b3549c",
   "metadata": {},
   "source": [
    "## DistilBERT Fine-tuning (Hugging Face)\n",
    "This section fine-tunes DistilBERT. **Warning:** fine-tuning on CPU is slow. For research, use GPU. We run 1 epoch here for a quick experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6b7a2dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch: 2.8.0\n",
      "Transformers: 4.57.0\n",
      "Datasets: 4.1.1\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import datasets\n",
    "import evaluate\n",
    "\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"Transformers:\", transformers.__version__)\n",
    "print(\"Datasets:\", datasets.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be91be70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47b26f2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed58107f5da042e399f64186d116903b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/40000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dab29860410f48bca6315abd39c2fb84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build HF Datasets\n",
    "train_df = pd.DataFrame({'text': X_train.values, 'label': y_train.values})\n",
    "test_df = pd.DataFrame({'text': X_test.values, 'label': y_test.values})\n",
    "train_ds = Dataset.from_pandas(train_df)\n",
    "test_ds = Dataset.from_pandas(test_df)\n",
    "\n",
    "model_name = 'distilbert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# tokenization function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=256)\n",
    "\n",
    "train_ds = train_ds.map(tokenize_function, batched=True)\n",
    "train_ds = train_ds.remove_columns(['text'])\n",
    "train_ds.set_format(type='torch', columns=['input_ids','attention_mask','label'])\n",
    "\n",
    "test_ds = test_ds.map(tokenize_function, batched=True)\n",
    "test_ds = test_ds.remove_columns(['text'])\n",
    "test_ds.set_format(type='torch', columns=['input_ids','attention_mask','label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534eba77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/ravina/Desktop/Text_Classification_Project/venv/lib/python3.11/site-packages (4.56.2)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.57.0-py3-none-any.whl.metadata (41 kB)\n",
      "Requirement already satisfied: filelock in /Users/ravina/Desktop/Text_Classification_Project/venv/lib/python3.11/site-packages (from transformers) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /Users/ravina/Desktop/Text_Classification_Project/venv/lib/python3.11/site-packages (from transformers) (0.35.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/ravina/Desktop/Text_Classification_Project/venv/lib/python3.11/site-packages (from transformers) (2.3.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/ravina/Desktop/Text_Classification_Project/venv/lib/python3.11/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/ravina/Desktop/Text_Classification_Project/venv/lib/python3.11/site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/ravina/Desktop/Text_Classification_Project/venv/lib/python3.11/site-packages (from transformers) (2025.9.18)\n",
      "Requirement already satisfied: requests in /Users/ravina/Desktop/Text_Classification_Project/venv/lib/python3.11/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /Users/ravina/Desktop/Text_Classification_Project/venv/lib/python3.11/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/ravina/Desktop/Text_Classification_Project/venv/lib/python3.11/site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/ravina/Desktop/Text_Classification_Project/venv/lib/python3.11/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/ravina/Desktop/Text_Classification_Project/venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/ravina/Desktop/Text_Classification_Project/venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/ravina/Desktop/Text_Classification_Project/venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/ravina/Desktop/Text_Classification_Project/venv/lib/python3.11/site-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ravina/Desktop/Text_Classification_Project/venv/lib/python3.11/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ravina/Desktop/Text_Classification_Project/venv/lib/python3.11/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ravina/Desktop/Text_Classification_Project/venv/lib/python3.11/site-packages (from requests->transformers) (2025.8.3)\n",
      "Downloading transformers-4.57.0-py3-none-any.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m  \u001b[33m0:00:04\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.56.2\n",
      "    Uninstalling transformers-4.56.2:\n",
      "      Successfully uninstalled transformers-4.56.2\n",
      "Successfully installed transformers-4.57.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "! pip install -U transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d04855d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.57.0\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f785646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ravina/Desktop/text_classification_project/venv/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9630afc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments\n",
    "from pathlib import Path\n",
    "\n",
    "RESULTS_DIR = Path(\"results\")\n",
    "\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=str(RESULTS_DIR / 'hf_outputs'),\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    eval_strategy='epoch',   # ✅ fixed keyword\n",
    "    save_strategy='no',\n",
    "    logging_steps=100,\n",
    "    fp16=False,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "281a9376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {'accuracy': acc, 'precision': precision, 'recall': recall, 'f1': f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3083418e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3t/5qtlv2451k3fkqv9ls22lmg00000gn/T/ipykernel_72238/1265177924.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e3d1a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ravina/Desktop/text_classification_project/venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5000' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5000/5000 3:39:52, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.250300</td>\n",
       "      <td>0.276019</td>\n",
       "      <td>0.899000</td>\n",
       "      <td>0.895676</td>\n",
       "      <td>0.903200</td>\n",
       "      <td>0.899422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ravina/Desktop/text_classification_project/venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='625' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [625/625 27:28]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBERT eval metrics: {'eval_loss': 0.276019424200058, 'eval_accuracy': 0.899, 'eval_precision': 0.895676318921063, 'eval_recall': 0.9032, 'eval_f1': 0.8994224258115913, 'eval_runtime': 1649.3018, 'eval_samples_per_second': 6.063, 'eval_steps_per_second': 0.379, 'epoch': 1.0}\n",
      "Saved DistilBERT to results/hf_outputs/distilbert_imdb\n"
     ]
    }
   ],
   "source": [
    "# Train (may be slow on CPU)\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate and save\n",
    "metrics = trainer.evaluate()\n",
    "print('DistilBERT eval metrics:', metrics)\n",
    "trainer.save_model(RESULTS_DIR / 'hf_outputs' / 'distilbert_imdb')\n",
    "print('Saved DistilBERT to', RESULTS_DIR / 'hf_outputs' / 'distilbert_imdb')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd803e39",
   "metadata": {},
   "source": [
    "## Comparison & Next Steps\n",
    "- Compare classical results (saved in `results/classical_results_summary.json`) with DistilBERT metrics printed above.\n",
    "- For full transformer training, use a GPU (Colab or cloud). Consider distillation for production.\n",
    "\n",
    "Files saved to `results/`:\n",
    "- TF-IDF vectorizer: `results/tfidf_vectorizer.joblib`\n",
    "- Classical models: `results/{model}.joblib`\n",
    "- Classical summary: `results/classical_results_summary.json`\n",
    "- DistilBERT outputs: `results/hf_outputs/distilbert_imdb`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70431ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP Env",
   "language": "python",
   "name": "nlpvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
